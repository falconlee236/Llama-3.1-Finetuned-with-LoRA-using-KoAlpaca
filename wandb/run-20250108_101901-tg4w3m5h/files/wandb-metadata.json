{
  "os":  "Linux-5.4.0-169-generic-x86_64-with-glibc2.35",
  "python":  "CPython 3.11.11",
  "startedAt":  "2025-01-08T10:19:01.912339Z",
  "program":  "/root/Llama-3.1-Finetuned-with-LoRA-using-KoAlpaca/train.py",
  "codePath":  "train.py",
  "git":  {
    "remote":  "https://github.com/falconlee236/Llama-3.1-Finetuned-with-LoRA-using-KoAlpaca",
    "commit":  "ad3f318296202b691a5427acedfb695bf0e5d064"
  },
  "email":  "falconlee236@gmail.com",
  "root":  "/root/Llama-3.1-Finetuned-with-LoRA-using-KoAlpaca",
  "host":  "a024af2f9c8a",
  "executable":  "/usr/bin/python",
  "codePathLocal":  "train.py",
  "cpu_count":  48,
  "cpu_count_logical":  96,
  "gpu":  "NVIDIA A40",
  "gpu_count":  2,
  "disk":  {
    "/":  {
      "total":  "107374182400",
      "used":  "51759620096"
    }
  },
  "memory":  {
    "total":  "540662652928"
  },
  "cpu":  {
    "count":  48,
    "countLogical":  96
  },
  "gpu_nvidia":  [
    {
      "name":  "NVIDIA A40",
      "memoryTotal":  "48305799168",
      "cudaCores":  10752,
      "architecture":  "Ampere"
    },
    {
      "name":  "NVIDIA A40",
      "memoryTotal":  "48305799168",
      "cudaCores":  10752,
      "architecture":  "Ampere"
    }
  ],
  "cudaVersion":  "12.4"
}