wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: falconlee236 (OptiMap). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc
DatasetDict({
    train: Dataset({
        features: ['instruction', 'output', 'url'],
        num_rows: 21155
    })
})
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.11s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.28s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.60s/it]
/root/Llama-3.1-Finetuned-with-LoRA-using-KoAlpaca/train.py:89: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.
  trainer = SFTTrainer(
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /root/Llama-3.1-Finetuned-with-LoRA-using-KoAlpaca/wandb/run-20250108_101901-tg4w3m5h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ./results
wandb: ⭐️ View project at https://wandb.ai/OptiMap/Llama-3.1-Finetuned-with-LoRA-using-KoAlpaca
wandb: 🚀 View run at https://wandb.ai/OptiMap/Llama-3.1-Finetuned-with-LoRA-using-KoAlpaca/runs/tg4w3m5h
  0%|          | 0/330 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  0%|          | 1/330 [00:08<48:24,  8.83s/it]  1%|          | 2/330 [00:24<1:10:23, 12.88s/it]  1%|          | 3/330 [00:36<1:08:29, 12.57s/it]  1%|          | 4/330 [00:49<1:08:00, 12.52s/it]  2%|▏         | 5/330 [00:58<1:02:15, 11.49s/it]  2%|▏         | 6/330 [01:06<54:28, 10.09s/it]    2%|▏         | 7/330 [01:16<54:58, 10.21s/it]  2%|▏         | 8/330 [01:26<54:01, 10.07s/it]  3%|▎         | 9/330 [01:36<54:14, 10.14s/it]  3%|▎         | 10/330 [01:46<53:31, 10.04s/it]  3%|▎         | 11/330 [01:57<55:33, 10.45s/it]  4%|▎         | 12/330 [02:08<55:31, 10.48s/it]  4%|▍         | 13/330 [02:19<55:56, 10.59s/it]  4%|▍         | 14/330 [02:30<56:47, 10.78s/it]  5%|▍         | 15/330 [02:41<57:02, 10.86s/it]  5%|▍         | 16/330 [02:50<53:13, 10.17s/it]  5%|▌         | 17/330 [03:05<1:01:12, 11.73s/it]  5%|▌         | 18/330 [03:13<54:32, 10.49s/it]    6%|▌         | 19/330 [03:22<53:21, 10.29s/it]  6%|▌         | 20/330 [03:34<55:39, 10.77s/it]  6%|▋         | 21/330 [03:45<55:47, 10.83s/it]